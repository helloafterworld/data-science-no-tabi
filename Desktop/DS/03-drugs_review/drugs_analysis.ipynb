{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51057a14",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab62414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Mengunduh paket 'punkt' yang diperlukan untuk tokenisasi\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Mengunduh paket 'stopwords'\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Mengunduh paket 'wordnet' untuk lemmatization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"Semua paket NLTK yang diperlukan telah siap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a53cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('Medicine_Details.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69038433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395edb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_manufacturers = df['Manufacturer'].value_counts().head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=top_15_manufacturers.values, y=top_15_manufacturers.index, palette='viridis')\n",
    "plt.title('Top 15 Produsen dengan Jumlah Produk Terbanyak')\n",
    "plt.xlabel('Jumlah Produk')\n",
    "plt.ylabel('Produsen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a8a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Mengambil semua zat aktif, membersihkan dosis, dan menghitungnya\n",
    "all_compositions = ' '.join(df['Composition'])\n",
    "# Menghapus info dosis seperti (500mg)\n",
    "cleaned_compositions = re.sub(r'\\\\s*\\\\(.*?\\\\)\\\\s*', ' ', all_compositions)\n",
    "composition_counts = Counter(cleaned_compositions.split(' + '))\n",
    "\n",
    "# Ubah ke DataFrame untuk visualisasi\n",
    "composition_df = pd.DataFrame(composition_counts.most_common(15), columns=['Zat Aktif', 'Jumlah'])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Jumlah', y='Zat Aktif', data=composition_df, palette='plasma')\n",
    "plt.title('Top 15 Zat Aktif yang Paling Umum Digunakan')\n",
    "plt.xlabel('Jumlah Kemunculan')\n",
    "plt.ylabel('Zat Aktif')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be707aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memisahkan setiap efek samping dan memasukkannya ke dalam satu list besar\n",
    "all_side_effects_list = []\n",
    "for index, row in df.iterrows():\n",
    "    effects = row['Side_effects'].split(' ')\n",
    "    # Menghapus kata-kata kosong yang mungkin muncul\n",
    "    effects = [effect for effect in effects if effect] \n",
    "    all_side_effects_list.extend(effects)\n",
    "\n",
    "# Menghitung frekuensi setiap efek samping\n",
    "side_effects_counts = Counter(all_side_effects_list)\n",
    "\n",
    "# Ubah ke DataFrame untuk visualisasi\n",
    "side_effects_df = pd.DataFrame(side_effects_counts.most_common(20), columns=['Efek Samping', 'Jumlah Laporan'])\n",
    "\n",
    "# Visualisasikan 20 efek samping teratas\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x='Jumlah Laporan', y='Efek Samping', data=side_effects_df, palette='rocket')\n",
    "plt.title('Top 20 Efek Samping yang Paling Sering Dilaporkan')\n",
    "plt.xlabel('Jumlah Laporan')\n",
    "plt.ylabel('Efek Samping')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Gabungkan semua teks efek samping menjadi satu teks besar\n",
    "side_effects_text = ' '.join(df['Side_effects'])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(side_effects_text)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off') # Hilangkan sumbu\n",
    "plt.title('Word Cloud dari Efek Samping yang Dilaporkan')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c085d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memisahkan setiap efek samping dan memasukkannya ke dalam satu list besar\n",
    "all_side_effects_list = []\n",
    "for index, row in df.iterrows():\n",
    "    effects = row['Side_effects'].split(' ')\n",
    "    # Menghapus kata-kata kosong yang mungkin muncul\n",
    "    effects = [effect for effect in effects if effect] \n",
    "    all_side_effects_list.extend(effects)\n",
    "\n",
    "# Menghitung frekuensi setiap efek samping\n",
    "side_effects_counts = Counter(all_side_effects_list)\n",
    "\n",
    "# Ubah ke DataFrame untuk visualisasi\n",
    "side_effects_df = pd.DataFrame(side_effects_counts.most_common(20), columns=['Efek Samping', 'Jumlah Laporan'])\n",
    "\n",
    "# Visualisasikan 20 efek samping teratas\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x='Jumlah Laporan', y='Efek Samping', data=side_effects_df, palette='rocket')\n",
    "plt.title('Top 20 Efek Samping yang Paling Sering Dilaporkan')\n",
    "plt.xlabel('Jumlah Laporan')\n",
    "plt.ylabel('Efek Samping')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6dfda",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b343e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Inisialisasi komponen NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Sebuah fungsi untuk membersihkan data teks melalui 5 langkah:\n",
    "    1. Lowercasing\n",
    "    2. Menghapus tanda baca dan angka\n",
    "    3. Tokenization\n",
    "    4. Menghapus stopwords\n",
    "    5. Lemmatization\n",
    "    \"\"\"\n",
    "    # 1. Lowercasing: Mengubah semua teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove Punctuation and Numbers: Hanya menyisakan huruf\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # 3. Tokenization: Memecah kalimat menjadi daftar kata\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 4. Remove Stopwords: Menghapus kata-kata umum seperti 'and', 'the', 'in'\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 5. Lemmatization: Mengubah kata ke bentuk dasarnya (misal: 'effects', 'effective' -> 'effect')\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    # Gabungkan kembali menjadi satu string teks yang bersih\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terapkan fungsi pada kolom 'Uses' dan simpan di kolom baru\n",
    "df['uses_cleaned'] = df['Uses'].apply(preprocess_text)\n",
    "\n",
    "# Terapkan fungsi pada kolom 'Side_effects' dan simpan di kolom baru\n",
    "df['side_effects_cleaned'] = df['Side_effects'].apply(preprocess_text)\n",
    "\n",
    "# Tampilkan hasilnya untuk melihat perbandingan antara teks asli dan teks bersih\n",
    "print(\"Contoh Hasil Preprocessing:\")\n",
    "print(df[['Uses', 'uses_cleaned', 'Side_effects', 'side_effects_cleaned']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b95a2f",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e48503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Inisialisasi TF-IDF Vectorizer\n",
    "# max_features=5000 berarti kita hanya akan mengambil 5000 kata paling penting untuk efisiensi\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Lakukan fit_transform untuk belajar kosakata dari 'uses_cleaned' dan mengubahnya menjadi matriks angka\n",
    "X = tfidf_vectorizer.fit_transform(df['uses_cleaned'])\n",
    "\n",
    "# Lihat bentuk (shape) dari matriks yang dihasilkan\n",
    "print(\"Bentuk (shape) dari matriks TF-IDF untuk 'Uses':\")\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a615009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment(row):\n",
    "    # Kita tetapkan margin 15% untuk menentukan sentimen yang kuat\n",
    "    if row['Excellent Review %'] > row['Poor Review %'] + 15:\n",
    "        return 'Positif'\n",
    "    elif row['Poor Review %'] > row['Excellent Review %'] + 15:\n",
    "        return 'Negatif'\n",
    "    else:\n",
    "        return 'Netral'\n",
    "\n",
    "# Terapkan fungsi ini untuk membuat kolom target 'sentiment'\n",
    "df['sentiment'] = df.apply(create_sentiment, axis=1)\n",
    "\n",
    "# Mari kita lihat distribusi sentimen yang berhasil kita buat\n",
    "print(\"\\nDistribusi Sentimen:\")\n",
    "sentiment_distribution = df['sentiment'].value_counts()\n",
    "print(sentiment_distribution)\n",
    "\n",
    "# Visualisasikan distribusinya\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=sentiment_distribution.index, y=sentiment_distribution.values)\n",
    "plt.title('Distribusi Sentimen Obat')\n",
    "plt.ylabel('Jumlah Obat')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20706285",
   "metadata": {},
   "source": [
    "## Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf935e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X adalah matriks TF-IDF Anda dari langkah sebelumnya\n",
    "# df['sentiment'] adalah kolom target Anda\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    df['sentiment'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['sentiment'] # Penting untuk menjaga proporsi sentimen yang seimbang\n",
    ")\n",
    "\n",
    "print(\"Data berhasil dibagi menjadi set latih dan uji.\")\n",
    "print(f\"Ukuran data latih: {X_train.shape[0]} baris\")\n",
    "print(f\"Ukuran data uji: {X_test.shape[0]} baris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inisialisasi dan latih model Naive Bayes\n",
    "print(\"Melatih model Multinomial Naive Bayes...\")\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "print(\"Model Naive Bayes berhasil dilatih.\")\n",
    "\n",
    "# Inisialisasi dan latih model Logistic Regression\n",
    "print(\"\\nMelatih model Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"Model Logistic Regression berhasil dilatih.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Lakukan prediksi pada data uji\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Tampilkan laporan evaluasi\n",
    "print(\"--- Laporan Evaluasi untuk Multinomial Naive Bayes ---\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "print(\"\\n--- Laporan Evaluasi untuk Logistic Regression ---\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pastikan 'tfidf_vectorizer' adalah objek TfidfVectorizer yang sudah Anda .fit_transform()\n",
    "# dan 'lr_model' adalah model Logistic Regression yang sudah Anda .fit()\n",
    "\n",
    "# Dapatkan nama-nama fitur (kata-kata) dari vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Dapatkan koefisien (bobot) dari model Logistic Regression\n",
    "# Karena ini multi-kelas, kita lihat koefisien untuk setiap kelas\n",
    "coefficients = lr_model.coef_\n",
    "\n",
    "# Buat DataFrame untuk melihat koefisien dengan lebih mudah\n",
    "coef_df = pd.DataFrame(coefficients, columns=feature_names, index=lr_model.classes_)\n",
    "\n",
    "# Mari kita lihat kata-kata yang paling berpengaruh\n",
    "print(\"--- Kata-kata Paling Berpengaruh ---\")\n",
    "\n",
    "# Kata-kata pendorong sentimen 'Positif'\n",
    "top_positive_words = coef_df.loc['Positif'].sort_values(ascending=False).head(15)\n",
    "print(\"\\nTop 15 Kata Pendorong Sentimen 'Positif':\")\n",
    "print(top_positive_words)\n",
    "\n",
    "# Kata-kata pendorong sentimen 'Negatif'\n",
    "top_negative_words = coef_df.loc['Negatif'].sort_values(ascending=False).head(15)\n",
    "print(\"\\nTop 15 Kata Pendorong Sentimen 'Negatif':\")\n",
    "print(top_negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04955bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# y_pred_lr adalah hasil prediksi dari model Logistic Regression Anda\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "class_names = ['Negatif', 'Netral', 'Positif'] # Sesuaikan urutan jika berbeda\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Prediksi Model')\n",
    "plt.ylabel('Aktual (Sebenarnya)')\n",
    "plt.title('Confusion Matrix untuk Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ddd22",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd19907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definisikan parameter yang ingin kita uji untuk Logistic Regression\n",
    "# 'C' adalah parameter kekuatan regularisasi\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Buat objek GridSearchCV\n",
    "# scoring='f1_macro' adalah metrik yang baik untuk masalah multi-kelas\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Latih pencarian pada data training Anda\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Tampilkan parameter terbaik\n",
    "print(f\"Parameter 'C' terbaik ditemukan: {grid_search.best_params_['C']}\")\n",
    "\n",
    "# Simpan model terbaik hasil tuning\n",
    "best_lr_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakukan prediksi dengan model terbaik\n",
    "y_pred_best = best_lr_model.predict(X_test)\n",
    "\n",
    "# Tampilkan laporan klasifikasi final\n",
    "print(\"--- Laporan Evaluasi Final untuk Model yang Dioptimalkan ---\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426dfcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk membersihkan data teks:\n",
    "    1. Mengubah ke huruf kecil (lowercasing).\n",
    "    2. Menghapus semua karakter non-alfabet (tanda baca, angka).\n",
    "    3. Memecah kalimat menjadi kata (tokenization).\n",
    "    4. Menghapus kata-kata umum (stopwords).\n",
    "    5. Mengubah kata ke bentuk dasarnya (lemmatization).\n",
    "    \"\"\"\n",
    "    # 1. Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 4. Remove Stopwords\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 5. Lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    # Gabungkan kembali menjadi satu string\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb168fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terapkan fungsi pada kolom 'review' untuk membuat kolom baru 'review_cleaned'\n",
    "# Mungkin butuh beberapa menit untuk berjalan\n",
    "df['review_cleaned'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# Tampilkan hasilnya untuk melihat perbandingan\n",
    "print(\"Contoh Hasil Preprocessing:\")\n",
    "print(df[['review', 'review_cleaned']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terapkan fungsi pada kolom 'review' untuk membuat kolom baru 'review_cleaned'\n",
    "# Mungkin butuh beberapa menit untuk berjalan\n",
    "df['review_cleaned'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# Tampilkan hasilnya untuk melihat perbandingan\n",
    "print(\"Contoh Hasil Preprocessing:\")\n",
    "print(df[['review', 'review_cleaned']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Gabungkan semua teks dari kolom yang sudah bersih\n",
    "all_reviews_cleaned = ' '.join(df['review_cleaned'])\n",
    "\n",
    "# Buat dan tampilkan word cloud\n",
    "wordcloud = WordCloud(width=1000, height=500, background_color='white', collocations=False).generate(all_reviews_cleaned)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud dari Ulasan yang Telah Dibersihkan')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
